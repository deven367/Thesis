{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load PMI matrix\n",
    "* Load dictionary\n",
    "* Calculate scores\n",
    "* Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_pmi(path):\n",
    "    pmi = np.load(path)\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_dictionary(path):\n",
    "    fname = open(path, 'rb')\n",
    "    data = pickle.load(fname)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score_v2(pmi, d, sentence_a, sentence_b):\n",
    "    s = 0\n",
    "    pairs = 0\n",
    "    for word_1 in sentence_a.split():\n",
    "        for word_2 in sentence_b.split():\n",
    "            x = d.token2id.get(word_1,-1)\n",
    "            y = d.token2id.get(word_2,-1)\n",
    "            if x == -1 or y == -1:\n",
    "                continue\n",
    "            s += pmi[x][y]\n",
    "            pairs += 1\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        c = np.true_divide(s, pairs)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_scores(pmi, d, all_sentences, k):\n",
    "    successive = []\n",
    "    for i in range(len(all_sentences) - k):\n",
    "        successive.append(calc_score_v2(pmi, d, all_sentences[i], all_sentences[i+k]))\n",
    "    return successive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(data):\n",
    "    return [line for line in data.split('\\n') if len(line)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        all = f.read()\n",
    "    return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(path):\n",
    "    for fx in os.listdir(path):\n",
    "        if fx.endswith('.pkl'):\n",
    "            d = load_dictionary(path+fx)\n",
    "            \n",
    "        if fx.endswith('.npy'):\n",
    "            pmi = load_pmi(path+fx)\n",
    "            \n",
    "        if fx.endswith('.txt'):\n",
    "            data = read_txt(path+fx)\n",
    "            sentences = get_all_sentences(data)\n",
    "            \n",
    "    return pmi, d, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(path, k):\n",
    "    pmi, d, sentences = iterator(path)\n",
    "    parent_dir = os.path.basename(os.path.dirname(path))\n",
    "    name = parent_dir + '_lexical.npy'\n",
    "    \n",
    "    lexical = get_all_scores(pmi, d, sentences, k)\n",
    "    np.save(path + name, lexical)\n",
    "    \n",
    "    name = name = parent_dir + '_lexical'\n",
    "    df = pd.DataFrame(lexical)\n",
    "    df.to_csv(path + name+'.csv')\n",
    "    print(\"Created {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_v2(fname):\n",
    "    all_data = get_data(fname)\n",
    "    all_data = unidecode.unidecode(all_data)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname)\n",
    "    return removed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a christmas carol_lexical\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/a christmas carol/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a tale of 2 cities_lexical.npy\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/a tale of 2 cities/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created great gatsby_lexical.npy\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/great gatsby/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metamorphosis_lexical\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/metamorphosis/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mysterious affair_lexical.npy\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/mysterious affair/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pride and prejudice_lexical.npy\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/pride and prejudice/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the prophet_lexical\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/the prophet/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created heart of darkness_lexical\n"
     ]
    }
   ],
   "source": [
    "path = '../final/lexical results/heart of darkness/'\n",
    "k = 1\n",
    "generator(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../final/lexical results/a christmas carol/'\n",
    "for fx in os.listdir(path):\n",
    "    if fx.endswith('.txt'):\n",
    "        data = read_txt(path+fx)\n",
    "        indices = process_v2()\n",
    "        sentences = get_all_sentences(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../final/lexical results/a christmas carol/'\n",
    "pmi_carol, d_carol, sentences_carol = iterator(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed = []\n",
    "for sentence in sentences_carol:\n",
    "    sentence_embed = []\n",
    "    for word in sentence.split():\n",
    "        word_index = d_carol.token2id.get(word, -1)\n",
    "        sentence_embed.append(pmi_carol[word_index])\n",
    "        \n",
    "#     print(len(sentence_embed))\n",
    "#     print(sentence_embed)\n",
    "    mat_ = np.asarray(sentence_embed)\n",
    "#     print(mat_)\n",
    "#     mean_ = np.mean(mat_, axis = 0)\n",
    "#     print(mean_.shape)\n",
    "    all_embed.append(np.mean(mat_, axis = 0 ))\n",
    "#     break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.6 MiB for an array with shape (1887, 3379) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-49b3a3bd0457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\deven mistry\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.6 MiB for an array with shape (1887, 3379) and data type float64"
     ]
    }
   ],
   "source": [
    "f = np.asarray(all_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54332534, 0.54332534, 0.54332534, ..., 0.13278206, 0.22103373,\n",
       "        0.22103373]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54332534, 0.54332534, 0.54332534, ..., 0.13278206, 0.22103373,\n",
       "        0.22103373],\n",
       "       [0.72336909, 0.72336909, 0.72336909, ..., 0.21725327, 0.29427851,\n",
       "        0.29427851],\n",
       "       [0.89235459, 0.89235459, 0.89235459, ..., 0.31240328, 0.36302461,\n",
       "        0.36302461],\n",
       "       ...,\n",
       "       [0.60295682, 0.60295682, 0.60295682, ..., 0.34615045, 0.24529281,\n",
       "        0.27651299],\n",
       "       [0.50169442, 0.50169442, 0.50169442, ..., 0.37951893, 0.20409759,\n",
       "        0.32273428],\n",
       "       [0.49132634, 0.49132634, 0.49132634, ..., 0.34309096, 0.19987968,\n",
       "        0.19987968]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.6 MiB for an array with shape (1887, 3379) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-d4cb27bf83a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\deven mistry\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deven mistry\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1702\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1704\u001b[1;33m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0m\u001b[0;32m   1705\u001b[0m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0;32m   1706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deven mistry\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     if (warn_on_dtype and dtypes_orig is not None and\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.6 MiB for an array with shape (1887, 3379) and data type float64"
     ]
    }
   ],
   "source": [
    "sns.heatmap(cosine_similarity(f,f), square = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
